{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corner</th>\n",
       "      <th>vsup</th>\n",
       "      <th>temperature</th>\n",
       "      <th>l55lp_v0101.lib.scs</th>\n",
       "      <th>l55lp_v0101.lib.scs.1</th>\n",
       "      <th>l55lp_v0101.lib.scs.2</th>\n",
       "      <th>Pass/Fail</th>\n",
       "      <th>VDC(\"/net13\")</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0_0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-55</td>\n",
       "      <td>ss_lp_io25</td>\n",
       "      <td>ff_lp_rvt12</td>\n",
       "      <td>ff_lp_bjt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0_1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-50</td>\n",
       "      <td>ss_lp_io25</td>\n",
       "      <td>ff_lp_rvt12</td>\n",
       "      <td>ff_lp_bjt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0_2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-40</td>\n",
       "      <td>ss_lp_io25</td>\n",
       "      <td>ff_lp_rvt12</td>\n",
       "      <td>ff_lp_bjt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0_3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-30</td>\n",
       "      <td>ss_lp_io25</td>\n",
       "      <td>ff_lp_rvt12</td>\n",
       "      <td>ff_lp_bjt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0_4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-20</td>\n",
       "      <td>ss_lp_io25</td>\n",
       "      <td>ff_lp_rvt12</td>\n",
       "      <td>ff_lp_bjt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Corner  vsup  temperature l55lp_v0101.lib.scs l55lp_v0101.lib.scs.1  \\\n",
       "0   C0_0   1.6          -55          ss_lp_io25           ff_lp_rvt12   \n",
       "1   C0_1   1.6          -50          ss_lp_io25           ff_lp_rvt12   \n",
       "2   C0_2   1.6          -40          ss_lp_io25           ff_lp_rvt12   \n",
       "3   C0_3   1.6          -30          ss_lp_io25           ff_lp_rvt12   \n",
       "4   C0_4   1.6          -20          ss_lp_io25           ff_lp_rvt12   \n",
       "\n",
       "  l55lp_v0101.lib.scs.2  Pass/Fail  VDC(\"/net13\")  \n",
       "0             ff_lp_bjt        NaN         0.6249  \n",
       "1             ff_lp_bjt        NaN         0.6136  \n",
       "2             ff_lp_bjt        NaN         0.5908  \n",
       "3             ff_lp_bjt        NaN         0.5680  \n",
       "4             ff_lp_bjt        NaN         0.5452  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./team_26.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOneHotEncoder(items) :\n",
    "    \n",
    "    # Finding  unique items\n",
    "    unique_items = list(set(items))\n",
    "\n",
    "    # Length of the given list\n",
    "    no_items = len(items)\n",
    "\n",
    "    # We will be returning a numpy array\n",
    "    encoded = np.zeros((no_items, 1))\n",
    "\n",
    "    for i, item in enumerate(items) :\n",
    "        encoded[i] = unique_items.index(item)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CadenceDataset(Dataset) :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        # Loading data\n",
    "        data = pd.read_csv('./team_26.csv')\n",
    "\n",
    "        # The following 5 values are the inputs\n",
    "        vsup = np.array(data['vsup']).reshape(-1, 1)\n",
    "        temperature = np.array(data['temperature']).reshape(-1, 1)\n",
    "        model1 = listOneHotEncoder(data['l55lp_v0101.lib.scs'])\n",
    "        model2 = listOneHotEncoder(data['l55lp_v0101.lib.scs.1'])\n",
    "        model3 = listOneHotEncoder(data['l55lp_v0101.lib.scs.2'])\n",
    "\n",
    "        # Concatenating them\n",
    "        self.x = np.concatenate((vsup, temperature, model1, model2, model3), axis=1)\n",
    "        \n",
    "        # Corresponding outputs\n",
    "        self.y = np.array(data['VDC(\"/net13\")']).reshape(-1, 1)\n",
    "\n",
    "        # Scaling the data\n",
    "        self.x = StandardScaler().fit_transform(self.x)\n",
    "\n",
    "        # Converting both to tensor\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.y = torch.from_numpy(self.y)\n",
    "\n",
    "        # Saving the number of samples\n",
    "        self.length = len(self.x)\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        return self.x[index].float(), self.y[index].float()\n",
    "\n",
    "    def __len__(self) :\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CadenceDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for training and testing\n",
    "train_dataset, test_dataset = random_split(dataset, [4131, 2754])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset[:][0]\n",
    "Y_train = train_dataset[:][1]\n",
    "\n",
    "X_test = test_dataset[:][0]\n",
    "Y_test = test_dataset[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearModel = LinearRegression()\n",
    "\n",
    "reg = LinearModel.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b) :\n",
    "    return np.mean((a-b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for the linear regression model is 0.8633065295726453\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy using R2 metric\n",
    "pred = reg.predict(X_test)\n",
    "\n",
    "print(f\"R2 score for the linear regression model is {r2_score(Y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Defining scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Defining criterion\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(optimizer, scheduler, criterion, epochs=10) :\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for data, labels in tqdm(train_dataloader):\n",
    "            # Transfer Data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                print(\"yes\")\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            \n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward Pass\n",
    "            target = model(data)\n",
    "            # Find the Loss\n",
    "            loss = criterion(target,labels)\n",
    "            # Calculate gradients \n",
    "            loss.backward()\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "            # Calculate Loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        # print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1033 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/vivek/CadenceRegressorEW2-main/model.ipynb Cell 18'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000017?line=0'>1</a>\u001b[0m train(optimizer, scheduler, criterion, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32m/home/vivek/CadenceRegressorEW2-main/model.ipynb Cell 17'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, scheduler, criterion, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000016?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000016?line=12'>13</a>\u001b[0m \u001b[39m# Forward Pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000016?line=13'>14</a>\u001b[0m target \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000016?line=14'>15</a>\u001b[0m \u001b[39m# Find the Loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vivek/CadenceRegressorEW2-main/model.ipynb#ch0000016?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(target,labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/vivek/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "train(optimizer, scheduler, criterion, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score for MLP is 0.994831244341355\n"
     ]
    }
   ],
   "source": [
    "# Finding out the predictions\n",
    "pred = model(X_test)\n",
    "\n",
    "# Calculating the R2 score\n",
    "print(f'R2 Score for MLP is {r2_score(Y_test.numpy(), pred.detach().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b276e8cdf7d33d4bba1d4a707c8c75d32d6389c11e4b0cafe5353d14e18b1fac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
